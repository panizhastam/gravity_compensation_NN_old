{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Creating a MLP regression model with PyTorch\n",
    "The first step is to import all the required dependencies. Then, we will use a Multilayer Perceptron based model, which is essentially a stack of layers containing neurons that can be trained.\n",
    "We also have to ensure that the dataset is prepared into a DataLoader, which ensures that data is shuffled and batched appropriately.\n",
    "Then, we pick a loss function and initialize it. We also init the model and the optimizer (Adam).\n",
    "Finally, we create the training loop, which effectively contains the high-level training process captured in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import MyData\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Loading and preparing the data\n",
    "personally I don't think normalization is needed for this project but here is the way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/newdataset.csv')\n",
    "# d = preprocessing.normalize(df, axis=0)\n",
    "# scaled_df = pd.DataFrame(d, columns=names)\n",
    "# scaled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Splitting the dataset into test and train with 20% to 80% ratio using scikit-learn API. Then, saving them separately to directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220436, 9)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "test.to_csv('dataset/testdata.csv', index=False)\n",
    "np.shape(test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting validation set from train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         8.590299999999999325e-02  3.558799999999999741e-01  \\\n",
      "595786                   0.404973                  1.202640   \n",
      "822041                   1.730303                  1.313080   \n",
      "359892                   2.116903                  0.601320   \n",
      "675018                   2.184403                  0.030680   \n",
      "206002                   0.619733                  0.454055   \n",
      "...                           ...                       ...   \n",
      "36465                    0.092039                  0.674950   \n",
      "1043488                  1.853003                  1.257860   \n",
      "559273                   0.085903                  0.355880   \n",
      "724376                   0.110447                  0.319064   \n",
      "139357                   2.184403                  0.625860   \n",
      "\n",
      "         2.454399999999999984e-02  0.000000000000000000e+00  \\\n",
      "595786                   0.079767                       0.0   \n",
      "822041                   1.116744                       0.0   \n",
      "359892                   0.190214                       0.0   \n",
      "675018                   1.165844                       1.0   \n",
      "206002                   1.460344                       1.0   \n",
      "...                           ...                       ...   \n",
      "36465                    0.613594                       0.0   \n",
      "1043488                  0.018408                       1.0   \n",
      "559273                   0.024544                       0.0   \n",
      "724376                   0.061360                       1.0   \n",
      "139357                   0.018408                       1.0   \n",
      "\n",
      "         0.000000000000000000e+00.1  0.000000000000000000e+00.2  \\\n",
      "595786                          0.0                         0.0   \n",
      "822041                          0.0                         0.0   \n",
      "359892                          0.0                         1.0   \n",
      "675018                          1.0                         1.0   \n",
      "206002                          0.0                         1.0   \n",
      "...                             ...                         ...   \n",
      "36465                           0.0                         0.0   \n",
      "1043488                         0.0                         0.0   \n",
      "559273                          0.0                         0.0   \n",
      "724376                          0.0                         1.0   \n",
      "139357                          1.0                         0.0   \n",
      "\n",
      "         0.000000000000000000e+00.3  0.000000000000000000e+00.4  \\\n",
      "595786                      91.3240                    197.3000   \n",
      "822041                     155.1600                    152.5100   \n",
      "359892                     204.9800                    114.2900   \n",
      "675018                     212.3800                      7.3608   \n",
      "206002                      78.6430                    158.2900   \n",
      "...                             ...                         ...   \n",
      "36465                       60.7920                    129.5900   \n",
      "1043488                    149.9400                    213.3800   \n",
      "559273                       2.1898                      7.2493   \n",
      "724376                      12.9620                    108.1500   \n",
      "139357                     221.7100                     72.0820   \n",
      "\n",
      "         0.000000000000000000e+00.5  \n",
      "595786                      93.5910  \n",
      "822041                     186.6200  \n",
      "359892                      56.7260  \n",
      "675018                     160.1200  \n",
      "206002                     193.8000  \n",
      "...                             ...  \n",
      "36465                      164.9800  \n",
      "1043488                     11.0550  \n",
      "559273                       2.3415  \n",
      "724376                      24.7060  \n",
      "139357                      37.9570  \n",
      "\n",
      "[176349 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "train, validation = train_test_split(train, test_size=0.2, random_state=42)\n",
    "train.to_csv('dataset/traindata.csv', index=False)\n",
    "validation.to_csv('dataset/validationdata.csv', index=False)\n",
    "np.shape(validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "(139357, slice(None, None, None))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\paniz\\anaconda3\\envs\\SoftExo\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\paniz\\anaconda3\\envs\\SoftExo\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\paniz\\anaconda3\\envs\\SoftExo\\lib\\site-packages\\pandas\\_libs\\index.pyx:144\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '(139357, slice(None, None, None))' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(validation[\u001b[39m139357\u001b[39;49m,:])\n",
      "File \u001b[1;32mc:\\Users\\paniz\\anaconda3\\envs\\SoftExo\\lib\\site-packages\\pandas\\core\\frame.py:3804\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3804\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3806\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\paniz\\anaconda3\\envs\\SoftExo\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3810\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3805\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3807\u001b[0m         \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m         \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m         \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m-> 3810\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_indexing_error(key)\n\u001b[0;32m   3811\u001b[0m         \u001b[39mraise\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[39m# GH#42269\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\paniz\\anaconda3\\envs\\SoftExo\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5966\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   5962\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_indexing_error\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[0;32m   5963\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_scalar(key):\n\u001b[0;32m   5964\u001b[0m         \u001b[39m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[0;32m   5965\u001b[0m         \u001b[39m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[1;32m-> 5966\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m: (139357, slice(None, None, None))"
     ]
    }
   ],
   "source": [
    "print(validation[139357,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Defining hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-6\n",
    "batch_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GainDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, file_name):\n",
    "        gain_df = pd.read_csv(file_name)\n",
    "\n",
    "        x = gain_df.iloc[:, 0:6].values\n",
    "        y = gain_df.iloc[:, 6:9].values\n",
    "\n",
    "        self.x_train = torch.tensor(x, dtype=torch.float32)\n",
    "        self.y_train = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_train[idx], self.y_train[idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Creating a loader for the test set which will read the data within batch size and put into memory.\n",
    "Note that each shuffle is set to false for the test loader.\n",
    "Here we also define inputs and outputs (features and lables). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensor shape in a training set is:  705400\n",
      "The tensor shape in a test set is:  220450\n",
      "The tensor shape in a valid set is:  176350\n"
     ]
    }
   ],
   "source": [
    "path = 'dataset/traindata.csv'\n",
    "train_data = GainDataset(path)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "print(\"The tensor shape in a training set is: \", len(train_loader) * batch_size)\n",
    "\n",
    "path = 'dataset/testdata.csv'\n",
    "test_data = GainDataset(path)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "print(\"The tensor shape in a test set is: \", len(test_loader) * batch_size)\n",
    "\n",
    "path = 'dataset/validationdata.csv'\n",
    "valid_data = GainDataset(path)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False)\n",
    "print(\"The tensor shape in a valid set is: \", len(valid_loader) * batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Defining the neural network\n",
    "Because we want the best fit on 6 inputs and 3 outputs, we will have two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_size = 6\n",
    "output_size = 3\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(input_size, 64)\n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        self.layer3 = nn.Linear(32, 16)\n",
    "        self.layer4 = nn.Linear(16, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.layer1(x))\n",
    "        x = nn.functional.relu(self.layer2(x))\n",
    "        x = nn.functional.relu(self.layer3(x))\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = Network(input_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Define your execution device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model will be running on cpu device\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (layer1): Linear(in_features=6, out_features=64, bias=True)\n",
       "  (layer2): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (layer3): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (layer4): Linear(in_features=16, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "print(\"The model will be running on\", device, \"device\\n\")\n",
    "model.to(device)  # Convert model parameters and buffers to CPU or Cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Function to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def saveModel():\n",
    "    path = \"trainedmodels/MLP2.pth\"\n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Define the loss function with Classification Cross-Entropy loss and an optimizer with Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_globvar():\n",
    "    global best_val_error    # Needed to modify global copy of best_val_error\n",
    "    best_val_error = 1e10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(num_epochs):\n",
    "    best_val_error = 1e10\n",
    "    print(\"Begin training...\")\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        running_train_loss = 0.0 \n",
    "        running_accuracy = 0.0 \n",
    "        running_vall_loss = 0.0 \n",
    "        total = 0 \n",
    "\n",
    "        # Training Loop\n",
    "        for data in train_loader:\n",
    "            # for data in enumerate(train_loader, 0):\n",
    "            inputs, outputs = data\n",
    "            optimizer.zero_grad()  # zero the parameter gradients\n",
    "            predicted_outputs = model(inputs)  # predict output from the model\n",
    "            train_loss = loss_fn(predicted_outputs, outputs)  # calculate loss for the predicted output\n",
    "            train_loss.backward()  # backpropagation\n",
    "            optimizer.step()  # adjust parameters based on the calculated gradients\n",
    "            # print('epoch {}, train loss {}'.format(epoch, train_loss.data))\n",
    "            running_train_loss += train_loss.item()  # track the loss value\n",
    "\n",
    "        # Calculate training loss value\n",
    "        train_loss_value = running_train_loss / len(train_loader)\n",
    "        # print('Completed training batch', epoch, 'Training Loss is: %.4f' % train_loss_value)\n",
    "        \n",
    "        # Validation Loop \n",
    "        with torch.no_grad(): \n",
    "            model.eval() \n",
    "            for data in valid_loader: \n",
    "               inputs, outputs = data \n",
    "               predicted_outputs = model(inputs) \n",
    "               val_loss = loss_fn(predicted_outputs, outputs) \n",
    "            #    print('epoch {}, validation loss {}'.format(epoch, val_loss.data))\n",
    "               running_vall_loss += val_loss.item()  \n",
    "               running_accuracy += running_vall_loss\n",
    " \n",
    "        # Calculate validation loss value \n",
    "        val_loss_value = running_vall_loss/len(valid_loader) \n",
    "        print('Completed validation batch', epoch, 'Validation Loss is: %.4f' % val_loss_value)\n",
    "        \n",
    "        # Calculate accuracy as the number of correct predictions in the validation batch divided by the total number of predictions done.  \n",
    "        accuracy = running_accuracy/len(valid_loader)\n",
    "    \n",
    "        # print(\"Average accuracy: %f\" % accuracy)\n",
    "        # print(\"Test count error: %f\" % val_loss_value/250)    \n",
    " \n",
    "        # Save the model if the accuracy is the best \n",
    "        if val_loss_value < best_val_error:\n",
    "            best_val_error = val_loss_value\n",
    "\n",
    "            saveModel()\n",
    "        # Print the statistics of the epoch\n",
    "        print('Completed training batch', epoch, 'Training Loss is: %.4f' % train_loss_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    # Load the model that we saved at the end of the training loop\n",
    "    model = Network(input_size, output_size)\n",
    "    path = \"trainedmodels/MLP2.pth\"\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    running_accuracy = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, outputs = data\n",
    "            outputs = outputs.to(torch.float32)\n",
    "            predicted_outputs = model(inputs)\n",
    "            error = predicted_outputs - outputs # honestly I dont know what to do with the error nor how to interpret it haha:))\n",
    "            running_accuracy = mean_squared_error(outputs, predicted_outputs)\n",
    "\n",
    "        print('Accuracy of the model based on the test set of', len(test_loader) * batch_size,\n",
    "              'inputs is: %d %%' % running_accuracy) # this should somehow be in percents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, we run the main program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training...\n",
      "epoch 1, train loss 20511.9609375\n",
      "epoch 1, train loss 18055.55078125\n",
      "epoch 1, train loss 17637.916015625\n",
      "epoch 1, train loss 19925.26953125\n",
      "epoch 1, train loss 22732.939453125\n",
      "epoch 1, train loss 21213.25\n",
      "epoch 1, train loss 19207.015625\n",
      "epoch 1, train loss 22522.279296875\n",
      "epoch 1, train loss 18670.869140625\n",
      "epoch 1, train loss 15138.236328125\n",
      "epoch 1, train loss 22238.359375\n",
      "epoch 1, train loss 17270.08984375\n",
      "epoch 1, train loss 18715.41796875\n",
      "epoch 1, train loss 20124.38671875\n",
      "epoch 1, train loss 17840.677734375\n",
      "epoch 1, train loss 20928.421875\n",
      "epoch 1, train loss 20155.58203125\n",
      "epoch 1, train loss 22658.283203125\n",
      "epoch 1, train loss 20032.80078125\n",
      "epoch 1, train loss 18703.59765625\n",
      "epoch 1, train loss 17032.33984375\n",
      "epoch 1, train loss 22303.076171875\n",
      "epoch 1, train loss 19071.095703125\n",
      "epoch 1, train loss 20618.91796875\n",
      "epoch 1, train loss 18373.373046875\n",
      "epoch 1, train loss 22388.029296875\n",
      "epoch 1, train loss 18764.265625\n",
      "epoch 1, train loss 25344.447265625\n",
      "epoch 1, train loss 17478.685546875\n",
      "epoch 1, train loss 18807.888671875\n",
      "epoch 1, train loss 19201.255859375\n",
      "epoch 1, train loss 19868.80859375\n",
      "epoch 1, train loss 21110.94921875\n",
      "epoch 1, train loss 21525.41796875\n",
      "epoch 1, train loss 20082.982421875\n",
      "epoch 1, train loss 22195.599609375\n",
      "epoch 1, train loss 23117.482421875\n",
      "epoch 1, train loss 16549.2734375\n",
      "epoch 1, train loss 17956.6171875\n",
      "epoch 1, train loss 21928.1484375\n",
      "epoch 1, train loss 18417.498046875\n",
      "epoch 1, train loss 22641.0703125\n",
      "epoch 1, train loss 21870.056640625\n",
      "epoch 1, train loss 22709.3984375\n",
      "epoch 1, train loss 21747.927734375\n",
      "epoch 1, train loss 21856.744140625\n",
      "epoch 1, train loss 21276.513671875\n",
      "epoch 1, train loss 17045.279296875\n",
      "epoch 1, train loss 23215.798828125\n",
      "epoch 1, train loss 18545.38671875\n",
      "epoch 1, train loss 21868.119140625\n",
      "epoch 1, train loss 18595.701171875\n",
      "epoch 1, train loss 24226.3359375\n",
      "epoch 1, train loss 20748.189453125\n",
      "epoch 1, train loss 22061.060546875\n",
      "epoch 1, train loss 18157.267578125\n",
      "epoch 1, train loss 17992.67578125\n",
      "epoch 1, train loss 20363.603515625\n",
      "epoch 1, train loss 21327.03515625\n",
      "epoch 1, train loss 19633.02734375\n",
      "epoch 1, train loss 19671.962890625\n",
      "epoch 1, train loss 18120.28515625\n",
      "epoch 1, train loss 19979.44140625\n",
      "epoch 1, train loss 20623.751953125\n",
      "epoch 1, train loss 21769.740234375\n",
      "epoch 1, train loss 21585.92578125\n",
      "epoch 1, train loss 24383.2109375\n",
      "epoch 1, train loss 18483.494140625\n",
      "epoch 1, train loss 19326.974609375\n",
      "epoch 1, train loss 20793.19921875\n",
      "epoch 1, train loss 18036.78125\n",
      "epoch 1, train loss 18831.16015625\n",
      "epoch 1, train loss 19567.5546875\n",
      "epoch 1, train loss 18417.63671875\n",
      "epoch 1, train loss 21443.62109375\n",
      "epoch 1, train loss 21128.595703125\n",
      "epoch 1, train loss 21092.744140625\n",
      "epoch 1, train loss 20046.744140625\n",
      "epoch 1, train loss 20571.642578125\n",
      "epoch 1, train loss 21712.505859375\n",
      "epoch 1, train loss 23046.5546875\n",
      "epoch 1, train loss 21079.603515625\n",
      "epoch 1, train loss 22794.177734375\n",
      "epoch 1, train loss 21341.865234375\n",
      "epoch 1, train loss 19381.583984375\n",
      "epoch 1, train loss 23751.798828125\n",
      "epoch 1, train loss 20482.8359375\n",
      "epoch 1, train loss 18315.1640625\n",
      "epoch 1, train loss 17669.66015625\n",
      "epoch 1, train loss 21122.716796875\n",
      "epoch 1, train loss 19106.31640625\n",
      "epoch 1, train loss 23301.623046875\n",
      "epoch 1, train loss 18619.328125\n",
      "epoch 1, train loss 16966.2109375\n",
      "epoch 1, train loss 16888.591796875\n",
      "epoch 1, train loss 18201.63671875\n",
      "epoch 1, train loss 19739.57421875\n",
      "epoch 1, train loss 22121.236328125\n",
      "epoch 1, train loss 18461.5703125\n",
      "epoch 1, train loss 22535.5703125\n",
      "epoch 1, train loss 21713.8046875\n",
      "epoch 1, train loss 21214.689453125\n",
      "epoch 1, train loss 21124.560546875\n",
      "epoch 1, train loss 23128.119140625\n",
      "epoch 1, train loss 18033.966796875\n",
      "epoch 1, train loss 18040.0703125\n",
      "epoch 1, train loss 19479.025390625\n",
      "epoch 1, train loss 18603.4140625\n",
      "epoch 1, train loss 19664.2265625\n",
      "epoch 1, train loss 18670.095703125\n",
      "epoch 1, train loss 19233.744140625\n",
      "epoch 1, train loss 21729.001953125\n",
      "epoch 1, train loss 18193.064453125\n",
      "epoch 1, train loss 23078.576171875\n",
      "epoch 1, train loss 23705.849609375\n",
      "epoch 1, train loss 13981.2353515625\n",
      "epoch 1, train loss 20957.1875\n",
      "epoch 1, train loss 23322.740234375\n",
      "epoch 1, train loss 20871.494140625\n",
      "epoch 1, train loss 20075.8125\n",
      "epoch 1, train loss 21508.150390625\n",
      "epoch 1, train loss 17689.416015625\n",
      "epoch 1, train loss 19737.17578125\n",
      "epoch 1, train loss 23232.1640625\n",
      "epoch 1, train loss 19867.400390625\n",
      "epoch 1, train loss 20786.080078125\n",
      "epoch 1, train loss 18227.87109375\n",
      "epoch 1, train loss 21142.30078125\n",
      "epoch 1, train loss 23572.59765625\n",
      "epoch 1, train loss 19337.7109375\n",
      "epoch 1, train loss 20424.47265625\n",
      "epoch 1, train loss 20245.517578125\n",
      "epoch 1, train loss 21218.4921875\n",
      "epoch 1, train loss 17327.359375\n",
      "epoch 1, train loss 19356.8984375\n",
      "epoch 1, train loss 19650.32421875\n",
      "epoch 1, train loss 18562.3203125\n",
      "epoch 1, train loss 18440.76171875\n",
      "epoch 1, train loss 17704.7109375\n",
      "epoch 1, train loss 20196.69140625\n",
      "epoch 1, train loss 19295.771484375\n",
      "epoch 1, train loss 21799.869140625\n",
      "epoch 1, train loss 17592.603515625\n",
      "epoch 1, train loss 21656.736328125\n",
      "epoch 1, train loss 22318.94921875\n",
      "epoch 1, train loss 21808.03125\n",
      "epoch 1, train loss 15128.9501953125\n",
      "epoch 1, train loss 23275.916015625\n",
      "epoch 1, train loss 21171.33203125\n",
      "epoch 1, train loss 18536.20703125\n",
      "epoch 1, train loss 20058.984375\n",
      "epoch 1, train loss 19529.638671875\n",
      "epoch 1, train loss 22347.2109375\n",
      "epoch 1, train loss 21353.00390625\n",
      "epoch 1, train loss 20933.876953125\n",
      "epoch 1, train loss 20125.423828125\n",
      "epoch 1, train loss 18686.41796875\n",
      "epoch 1, train loss 20940.62890625\n",
      "epoch 1, train loss 19075.2109375\n",
      "epoch 1, train loss 18351.572265625\n",
      "epoch 1, train loss 21728.689453125\n",
      "epoch 1, train loss 22063.404296875\n",
      "epoch 1, train loss 18240.84765625\n",
      "epoch 1, train loss 19131.1875\n",
      "epoch 1, train loss 17242.021484375\n",
      "epoch 1, train loss 19513.626953125\n",
      "epoch 1, train loss 17224.376953125\n",
      "epoch 1, train loss 19439.15234375\n",
      "epoch 1, train loss 18016.744140625\n",
      "epoch 1, train loss 20504.90234375\n",
      "epoch 1, train loss 21284.884765625\n",
      "epoch 1, train loss 22005.775390625\n",
      "epoch 1, train loss 21339.896484375\n",
      "epoch 1, train loss 17683.779296875\n",
      "epoch 1, train loss 20081.76953125\n",
      "epoch 1, train loss 20747.828125\n",
      "epoch 1, train loss 20813.384765625\n",
      "epoch 1, train loss 20124.845703125\n",
      "epoch 1, train loss 20631.720703125\n",
      "epoch 1, train loss 17396.185546875\n",
      "epoch 1, train loss 14935.4833984375\n",
      "epoch 1, train loss 19770.4140625\n",
      "epoch 1, train loss 19140.19140625\n",
      "epoch 1, train loss 21415.90234375\n",
      "epoch 1, train loss 21534.498046875\n",
      "epoch 1, train loss 19559.25390625\n",
      "epoch 1, train loss 19927.169921875\n",
      "epoch 1, train loss 18861.595703125\n",
      "epoch 1, train loss 20494.513671875\n",
      "epoch 1, train loss 17145.75\n",
      "epoch 1, train loss 19798.88671875\n",
      "epoch 1, train loss 19865.955078125\n",
      "epoch 1, train loss 19459.3203125\n",
      "epoch 1, train loss 22621.078125\n",
      "epoch 1, train loss 19686.7734375\n",
      "epoch 1, train loss 20053.556640625\n",
      "epoch 1, train loss 17510.630859375\n",
      "epoch 1, train loss 20568.265625\n",
      "epoch 1, train loss 21165.2421875\n",
      "epoch 1, train loss 21519.654296875\n",
      "epoch 1, train loss 21521.880859375\n",
      "epoch 1, train loss 21384.0\n",
      "epoch 1, train loss 21624.94140625\n",
      "epoch 1, train loss 18706.296875\n",
      "epoch 1, train loss 21721.005859375\n",
      "epoch 1, train loss 23001.021484375\n",
      "epoch 1, train loss 18242.19921875\n",
      "epoch 1, train loss 17620.21875\n",
      "epoch 1, train loss 21757.349609375\n",
      "epoch 1, train loss 16924.59375\n",
      "epoch 1, train loss 18501.51171875\n",
      "epoch 1, train loss 16719.05078125\n",
      "epoch 1, train loss 23480.939453125\n",
      "epoch 1, train loss 20285.845703125\n",
      "epoch 1, train loss 23426.833984375\n",
      "epoch 1, train loss 21714.603515625\n",
      "epoch 1, train loss 19100.80078125\n",
      "epoch 1, train loss 20730.095703125\n",
      "epoch 1, train loss 17429.4609375\n",
      "epoch 1, train loss 18373.337890625\n",
      "epoch 1, train loss 23327.142578125\n",
      "epoch 1, train loss 18126.142578125\n",
      "epoch 1, train loss 21112.619140625\n",
      "epoch 1, train loss 20688.541015625\n",
      "epoch 1, train loss 17919.5390625\n",
      "epoch 1, train loss 18405.29296875\n",
      "epoch 1, train loss 19559.916015625\n",
      "epoch 1, train loss 21181.767578125\n",
      "epoch 1, train loss 20621.05078125\n",
      "epoch 1, train loss 18846.509765625\n",
      "epoch 1, train loss 20618.345703125\n",
      "epoch 1, train loss 23226.265625\n",
      "epoch 1, train loss 21300.134765625\n",
      "epoch 1, train loss 22716.42578125\n",
      "epoch 1, train loss 20793.8671875\n",
      "epoch 1, train loss 20232.419921875\n",
      "epoch 1, train loss 21041.341796875\n",
      "epoch 1, train loss 21806.50390625\n",
      "epoch 1, train loss 24236.33203125\n",
      "epoch 1, train loss 21987.267578125\n",
      "epoch 1, train loss 19641.634765625\n",
      "epoch 1, train loss 15980.1416015625\n",
      "epoch 1, train loss 19083.408203125\n",
      "epoch 1, train loss 21959.333984375\n",
      "epoch 1, train loss 20745.703125\n",
      "epoch 1, train loss 21768.54296875\n",
      "epoch 1, train loss 18679.171875\n",
      "epoch 1, train loss 20233.740234375\n",
      "epoch 1, train loss 21441.8359375\n",
      "epoch 1, train loss 18948.150390625\n",
      "epoch 1, train loss 22921.515625\n",
      "epoch 1, train loss 18057.6015625\n",
      "epoch 1, train loss 20076.14453125\n",
      "epoch 1, train loss 20581.2578125\n",
      "epoch 1, train loss 20252.103515625\n",
      "epoch 1, train loss 22068.744140625\n",
      "epoch 1, train loss 21534.154296875\n",
      "epoch 1, train loss 21222.6015625\n",
      "epoch 1, train loss 22027.060546875\n",
      "epoch 1, train loss 18854.32421875\n",
      "epoch 1, train loss 17010.490234375\n",
      "epoch 1, train loss 20449.591796875\n",
      "epoch 1, train loss 22928.826171875\n",
      "epoch 1, train loss 22748.11328125\n",
      "epoch 1, train loss 21382.123046875\n",
      "epoch 1, train loss 17003.80078125\n",
      "epoch 1, train loss 19649.4375\n",
      "epoch 1, train loss 18580.564453125\n",
      "epoch 1, train loss 21890.958984375\n",
      "epoch 1, train loss 24734.3671875\n",
      "epoch 1, train loss 19665.92578125\n",
      "epoch 1, train loss 23621.619140625\n",
      "epoch 1, train loss 23413.826171875\n",
      "epoch 1, train loss 19091.666015625\n",
      "epoch 1, train loss 21323.390625\n",
      "epoch 1, train loss 20682.03125\n",
      "epoch 1, train loss 20378.61328125\n",
      "epoch 1, train loss 22845.5859375\n",
      "epoch 1, train loss 22271.658203125\n",
      "epoch 1, train loss 21723.4375\n",
      "epoch 1, train loss 17678.82421875\n",
      "epoch 1, train loss 20768.416015625\n",
      "epoch 1, train loss 21098.931640625\n",
      "epoch 1, train loss 17171.240234375\n",
      "epoch 1, train loss 22598.259765625\n",
      "epoch 1, train loss 17971.970703125\n",
      "epoch 1, train loss 19992.05078125\n",
      "epoch 1, train loss 18743.703125\n",
      "epoch 1, train loss 22623.15234375\n",
      "epoch 1, train loss 20791.009765625\n",
      "epoch 1, train loss 22255.4375\n",
      "epoch 1, train loss 18615.140625\n",
      "epoch 1, train loss 19247.314453125\n",
      "epoch 1, train loss 17848.17578125\n",
      "epoch 1, train loss 16736.43359375\n",
      "epoch 1, train loss 20362.255859375\n",
      "epoch 1, train loss 19777.27734375\n",
      "epoch 1, train loss 17965.294921875\n",
      "epoch 1, train loss 18046.869140625\n",
      "epoch 1, train loss 19009.150390625\n",
      "epoch 1, train loss 21301.564453125\n",
      "epoch 1, train loss 21557.1328125\n",
      "epoch 1, train loss 22297.6015625\n",
      "epoch 1, train loss 21487.037109375\n",
      "epoch 1, train loss 18471.830078125\n",
      "epoch 1, train loss 20573.021484375\n",
      "epoch 1, train loss 18855.279296875\n",
      "epoch 1, train loss 20787.853515625\n",
      "epoch 1, train loss 17561.185546875\n",
      "epoch 1, train loss 20849.505859375\n",
      "epoch 1, train loss 20974.76171875\n",
      "epoch 1, train loss 18185.9375\n",
      "epoch 1, train loss 18726.833984375\n",
      "epoch 1, train loss 20179.3984375\n",
      "epoch 1, train loss 21795.626953125\n",
      "epoch 1, train loss 20190.884765625\n",
      "epoch 1, train loss 21069.5625\n",
      "epoch 1, train loss 20795.546875\n",
      "epoch 1, train loss 20275.76171875\n",
      "epoch 1, train loss 18328.8359375\n",
      "epoch 1, train loss 23438.71484375\n",
      "epoch 1, train loss 21163.849609375\n",
      "epoch 1, train loss 19396.2890625\n",
      "epoch 1, train loss 22853.576171875\n",
      "epoch 1, train loss 20831.390625\n",
      "epoch 1, train loss 17672.748046875\n",
      "epoch 1, train loss 18995.142578125\n",
      "epoch 1, train loss 20871.1875\n",
      "epoch 1, train loss 21646.220703125\n",
      "epoch 1, train loss 22588.015625\n",
      "epoch 1, train loss 22595.23046875\n",
      "epoch 1, train loss 21813.630859375\n",
      "epoch 1, train loss 19984.4375\n",
      "epoch 1, train loss 20339.005859375\n",
      "epoch 1, train loss 23245.666015625\n",
      "epoch 1, train loss 21223.4296875\n",
      "epoch 1, train loss 18274.998046875\n",
      "epoch 1, train loss 25599.83984375\n",
      "epoch 1, train loss 19033.927734375\n",
      "epoch 1, train loss 18795.48046875\n",
      "epoch 1, train loss 21648.138671875\n",
      "epoch 1, train loss 19295.09375\n",
      "epoch 1, train loss 18464.162109375\n",
      "epoch 1, train loss 16872.015625\n",
      "epoch 1, train loss 18427.626953125\n",
      "epoch 1, train loss 23183.94921875\n",
      "epoch 1, train loss 23634.5859375\n",
      "epoch 1, train loss 24259.470703125\n",
      "epoch 1, train loss 19251.6796875\n",
      "epoch 1, train loss 19408.94921875\n",
      "epoch 1, train loss 19397.70703125\n",
      "epoch 1, train loss 19420.060546875\n",
      "epoch 1, train loss 19491.095703125\n",
      "epoch 1, train loss 17632.15234375\n",
      "epoch 1, train loss 21510.43359375\n",
      "epoch 1, train loss 18627.11328125\n",
      "epoch 1, train loss 19288.67578125\n",
      "epoch 1, train loss 20576.2109375\n",
      "epoch 1, train loss 17031.07421875\n",
      "epoch 1, train loss 20822.9375\n",
      "epoch 1, train loss 18084.689453125\n",
      "epoch 1, train loss 20506.41796875\n",
      "epoch 1, train loss 20110.587890625\n",
      "epoch 1, train loss 22572.095703125\n",
      "epoch 1, train loss 20530.251953125\n",
      "epoch 1, train loss 16091.14453125\n",
      "epoch 1, train loss 22284.787109375\n",
      "epoch 1, train loss 21644.9140625\n",
      "epoch 1, train loss 21460.833984375\n",
      "epoch 1, train loss 21366.20703125\n",
      "epoch 1, train loss 17750.3125\n",
      "epoch 1, train loss 17813.3671875\n",
      "epoch 1, train loss 20770.92578125\n",
      "epoch 1, train loss 20513.638671875\n",
      "epoch 1, train loss 15377.5234375\n",
      "epoch 1, train loss 20322.76953125\n",
      "epoch 1, train loss 20970.8359375\n",
      "epoch 1, train loss 21592.634765625\n",
      "epoch 1, train loss 22894.5\n",
      "epoch 1, train loss 22974.94140625\n",
      "epoch 1, train loss 21984.259765625\n",
      "epoch 1, train loss 19361.48828125\n",
      "epoch 1, train loss 20819.283203125\n",
      "epoch 1, train loss 21254.9140625\n",
      "epoch 1, train loss 18732.533203125\n",
      "epoch 1, train loss 20676.4375\n",
      "epoch 1, train loss 18677.4921875\n",
      "epoch 1, train loss 18873.220703125\n",
      "epoch 1, train loss 18444.453125\n",
      "epoch 1, train loss 21178.474609375\n",
      "epoch 1, train loss 22566.216796875\n",
      "epoch 1, train loss 22237.169921875\n",
      "epoch 1, train loss 19493.099609375\n",
      "epoch 1, train loss 18268.875\n",
      "epoch 1, train loss 18837.5078125\n",
      "epoch 1, train loss 20005.513671875\n",
      "epoch 1, train loss 21359.453125\n",
      "epoch 1, train loss 16526.1015625\n",
      "epoch 1, train loss 17163.662109375\n",
      "epoch 1, train loss 26251.322265625\n",
      "epoch 1, train loss 21326.6796875\n",
      "epoch 1, train loss 17736.59375\n",
      "epoch 1, train loss 18759.41796875\n",
      "epoch 1, train loss 20720.55078125\n",
      "epoch 1, train loss 19313.775390625\n",
      "epoch 1, train loss 18380.587890625\n",
      "epoch 1, train loss 17321.5703125\n",
      "epoch 1, train loss 18504.298828125\n",
      "epoch 1, train loss 21032.861328125\n",
      "epoch 1, train loss 15966.1435546875\n",
      "epoch 1, train loss 18759.908203125\n",
      "epoch 1, train loss 21067.142578125\n",
      "epoch 1, train loss 23510.361328125\n",
      "epoch 1, train loss 18490.96875\n",
      "epoch 1, train loss 20648.076171875\n",
      "epoch 1, train loss 16556.326171875\n",
      "epoch 1, train loss 21331.029296875\n",
      "epoch 1, train loss 19238.263671875\n",
      "epoch 1, train loss 18215.970703125\n",
      "epoch 1, train loss 26753.703125\n",
      "epoch 1, train loss 19730.515625\n",
      "epoch 1, train loss 18799.095703125\n",
      "epoch 1, train loss 20475.212890625\n",
      "epoch 1, train loss 16028.287109375\n",
      "epoch 1, train loss 23418.013671875\n",
      "epoch 1, train loss 21573.244140625\n",
      "epoch 1, train loss 23788.369140625\n",
      "epoch 1, train loss 21618.396484375\n",
      "epoch 1, train loss 20805.5\n",
      "epoch 1, train loss 17016.94921875\n",
      "epoch 1, train loss 19203.03515625\n",
      "epoch 1, train loss 19911.505859375\n",
      "epoch 1, train loss 20432.57421875\n",
      "epoch 1, train loss 23400.224609375\n",
      "epoch 1, train loss 20755.755859375\n",
      "epoch 1, train loss 23174.22265625\n",
      "epoch 1, train loss 21275.00390625\n",
      "epoch 1, train loss 22481.59375\n",
      "epoch 1, train loss 23141.595703125\n",
      "epoch 1, train loss 19989.396484375\n",
      "epoch 1, train loss 19585.2734375\n",
      "epoch 1, train loss 17570.5234375\n",
      "epoch 1, train loss 19823.4296875\n",
      "epoch 1, train loss 23540.72265625\n",
      "epoch 1, train loss 22504.4140625\n",
      "epoch 1, train loss 20081.30078125\n",
      "epoch 1, train loss 18924.515625\n",
      "epoch 1, train loss 19664.17578125\n",
      "epoch 1, train loss 18622.07421875\n",
      "epoch 1, train loss 16472.8203125\n",
      "epoch 1, train loss 19512.986328125\n",
      "epoch 1, train loss 15963.501953125\n",
      "epoch 1, train loss 17724.88671875\n",
      "epoch 1, train loss 21250.197265625\n",
      "epoch 1, train loss 19442.373046875\n",
      "epoch 1, train loss 24404.505859375\n",
      "epoch 1, train loss 19634.265625\n",
      "epoch 1, train loss 21108.4765625\n",
      "epoch 1, train loss 20459.1640625\n",
      "epoch 1, train loss 19075.916015625\n",
      "epoch 1, train loss 18056.73828125\n",
      "epoch 1, train loss 20748.578125\n",
      "epoch 1, train loss 17249.240234375\n",
      "epoch 1, train loss 20796.61328125\n",
      "epoch 1, train loss 18261.61328125\n",
      "epoch 1, train loss 18817.5390625\n",
      "epoch 1, train loss 18423.134765625\n",
      "epoch 1, train loss 20100.111328125\n",
      "epoch 1, train loss 16836.14453125\n",
      "epoch 1, train loss 16661.849609375\n",
      "epoch 1, train loss 22822.64453125\n",
      "epoch 1, train loss 23610.810546875\n",
      "epoch 1, train loss 19390.05859375\n",
      "epoch 1, train loss 20825.658203125\n",
      "epoch 1, train loss 21580.1640625\n",
      "epoch 1, train loss 22390.654296875\n",
      "epoch 1, train loss 19196.697265625\n",
      "epoch 1, train loss 18529.103515625\n",
      "epoch 1, train loss 19607.6640625\n",
      "epoch 1, train loss 20617.314453125\n",
      "epoch 1, train loss 20971.20703125\n",
      "epoch 1, train loss 21130.779296875\n",
      "epoch 1, train loss 20742.529296875\n",
      "epoch 1, train loss 16821.42578125\n",
      "epoch 1, train loss 20445.513671875\n",
      "epoch 1, train loss 23232.244140625\n",
      "epoch 1, train loss 19510.51171875\n",
      "epoch 1, train loss 16497.494140625\n",
      "epoch 1, train loss 22895.724609375\n",
      "epoch 1, train loss 19420.2734375\n",
      "epoch 1, train loss 17759.1875\n",
      "epoch 1, train loss 21112.96875\n",
      "epoch 1, train loss 21007.3515625\n",
      "epoch 1, train loss 20936.1796875\n",
      "epoch 1, train loss 23211.23046875\n",
      "epoch 1, train loss 25101.53125\n",
      "epoch 1, train loss 21208.83984375\n",
      "epoch 1, train loss 23433.029296875\n",
      "epoch 1, train loss 21585.123046875\n",
      "epoch 1, train loss 19763.7109375\n",
      "epoch 1, train loss 18015.837890625\n",
      "epoch 1, train loss 22771.0703125\n",
      "epoch 1, train loss 20275.736328125\n",
      "epoch 1, train loss 28256.783203125\n",
      "epoch 1, train loss 21839.330078125\n",
      "epoch 1, train loss 16150.1865234375\n",
      "epoch 1, train loss 17986.44921875\n",
      "epoch 1, train loss 23595.9609375\n",
      "epoch 1, train loss 19732.595703125\n",
      "epoch 1, train loss 20338.029296875\n",
      "epoch 1, train loss 22030.599609375\n",
      "epoch 1, train loss 19083.0390625\n",
      "epoch 1, train loss 17931.0859375\n",
      "epoch 1, train loss 17063.267578125\n",
      "epoch 1, train loss 20954.279296875\n",
      "epoch 1, train loss 18817.869140625\n",
      "epoch 1, train loss 18919.11328125\n",
      "epoch 1, train loss 18943.650390625\n",
      "epoch 1, train loss 20261.4375\n",
      "epoch 1, train loss 16862.873046875\n",
      "epoch 1, train loss 18639.150390625\n",
      "epoch 1, train loss 18295.76171875\n",
      "epoch 1, train loss 19212.7109375\n",
      "epoch 1, train loss 20726.845703125\n",
      "epoch 1, train loss 19703.83203125\n",
      "epoch 1, train loss 19412.845703125\n",
      "epoch 1, train loss 18993.328125\n",
      "epoch 1, train loss 20953.79296875\n",
      "epoch 1, train loss 19124.35546875\n",
      "epoch 1, train loss 19494.826171875\n",
      "epoch 1, train loss 18394.947265625\n",
      "epoch 1, train loss 17512.720703125\n",
      "epoch 1, train loss 20470.607421875\n",
      "epoch 1, train loss 22662.84375\n",
      "epoch 1, train loss 20595.28125\n",
      "epoch 1, train loss 20158.064453125\n",
      "epoch 1, train loss 20183.380859375\n",
      "epoch 1, train loss 22019.7734375\n",
      "epoch 1, train loss 20528.3359375\n",
      "epoch 1, train loss 19597.982421875\n",
      "epoch 1, train loss 26132.4453125\n",
      "epoch 1, train loss 19558.84765625\n",
      "epoch 1, train loss 22151.3359375\n",
      "epoch 1, train loss 20884.978515625\n",
      "epoch 1, train loss 20586.271484375\n",
      "epoch 1, train loss 15335.4619140625\n",
      "epoch 1, train loss 21242.552734375\n",
      "epoch 1, train loss 21866.04296875\n",
      "epoch 1, train loss 18447.849609375\n",
      "epoch 1, train loss 18951.142578125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m torch\u001b[39m.\u001b[39mmanual_seed(\u001b[39m42\u001b[39m)\n\u001b[0;32m      3\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m60\u001b[39m\n\u001b[1;32m----> 4\u001b[0m train(num_epochs)\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFinished Training\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m test()\n",
      "Cell \u001b[1;32mIn[20], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(num_epochs)\u001b[0m\n\u001b[0;32m     15\u001b[0m predicted_outputs \u001b[39m=\u001b[39m model(inputs)  \u001b[39m# predict output from the model\u001b[39;00m\n\u001b[0;32m     16\u001b[0m train_loss \u001b[39m=\u001b[39m loss_fn(predicted_outputs, outputs)  \u001b[39m# calculate loss for the predicted output\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m train_loss\u001b[39m.\u001b[39;49mbackward()  \u001b[39m# backpropagation\u001b[39;00m\n\u001b[0;32m     18\u001b[0m optimizer\u001b[39m.\u001b[39mstep()  \u001b[39m# adjust parameters based on the calculated gradients\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mepoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, train loss \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch, train_loss\u001b[39m.\u001b[39mdata))\n",
      "File \u001b[1;32mc:\\Users\\paniz\\anaconda3\\envs\\SoftExo\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\paniz\\anaconda3\\envs\\SoftExo\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "    num_epochs = 60\n",
    "    train(num_epochs)\n",
    "    print('Finished Training\\n')\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Loading and using the model\n",
    "** Please refer to gettingoutputs.py **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16 (default, Jan 17 2023, 22:25:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b2c1f906ae4d0c458641dfb86e6b52d9c2b36c69ff426fd5e0a52ddb9fb4085e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
